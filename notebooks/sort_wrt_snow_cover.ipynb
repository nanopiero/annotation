{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "f95d9a2d-5843-4f86-a56c-ea5a12c3097b",
      "metadata": {
        "id": "f95d9a2d-5843-4f86-a56c-ea5a12c3097b"
      },
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "from os.path import join, isdir, isfile\n",
        "from os import listdir as ls\n",
        "import copy\n",
        "import os\n",
        "from IPython.display import display\n",
        "from ipywidgets import interact, widgets\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "import shutil\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Local clone\n",
        "! git clone https://github.com/nanopiero/annotation.git\n",
        "import sys\n",
        "sys.path.append('annotation')\n"
      ],
      "metadata": {
        "id": "Lz52xvY0FPoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8aea3e5-f9bb-4212-e597-bbedcd871797"
      },
      "id": "Lz52xvY0FPoz",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'annotation'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 54 (delta 29), reused 28 (delta 12), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (54/54), 253.53 KiB | 6.50 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the functions we need to sort a sequence :\n",
        "import importlib\n",
        "import src.utils as utils\n",
        "import src.poset_mergesort_functions_per_level  as pmf"
      ],
      "metadata": {
        "id": "f0-l8pOULwaq"
      },
      "id": "f0-l8pOULwaq",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a sequence to sort :"
      ],
      "metadata": {
        "id": "CloKZ43UQooQ"
      },
      "id": "CloKZ43UQooQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install huggingface_hub\n",
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "xZg8RH3CETfa"
      },
      "id": "xZg8RH3CETfa",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Step 1: Download the zip file using hf_hub_download\n",
        "local_zip = hf_hub_download(repo_id=\"nanopiero/weow\", filename=\"webcam_images.zip\")\n",
        "\n",
        "# Step 2: Unzip the contents\n",
        "with zipfile.ZipFile(local_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('webcam_images')"
      ],
      "metadata": {
        "id": "OZ_AK8gFTAkx"
      },
      "id": "OZ_AK8gFTAkx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all .jpg images in the directory\n",
        "image_dir = 'webcam_images'\n",
        "image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')], key=lambda x: int(x.split('.')[0]))\n",
        "\n",
        "def show_image(index):\n",
        "    img_path = os.path.join('webcam_images', image_files[index])\n",
        "    img = Image.open(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "# Use a slider to scroll through the images\n",
        "interact(show_image, index=widgets.IntSlider(min=0, max=len(image_files)-1, step=1, description=\"Image Index\"));\n"
      ],
      "metadata": {
        "id": "tYNFFyQmH1Ji"
      },
      "id": "tYNFFyQmH1Ji",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the folder for the sequence \"0000\":\n",
        "! mkdir -p dataset/0000/labels_ord dataset/0000/images\n",
        "! cp webcam_images/* dataset/0000/images/\n",
        "# ! ls dataset/0000/images"
      ],
      "metadata": {
        "id": "HbOX2v25Q85J"
      },
      "id": "HbOX2v25Q85J",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dictionnary of snow_cover levels\n",
        "# resulting from the image wise annotation step:\n",
        "# (here, the dict is arbitrarily set to the level '0' for all images)\n",
        "lbls = {n: {'sequence':'0000', 'levelsc':'0'} for n in ls('dataset/0000/images')}"
      ],
      "metadata": {
        "id": "BRcj5C6ZScko"
      },
      "id": "BRcj5C6ZScko",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config:\n",
        "critere = 'sc'\n",
        "param = 'sc'\n",
        "mode = 'surface'\n",
        "modes = [mode]\n",
        "subgroup = ''\n",
        "times = ['']\n",
        "\n",
        "grounds = [r'snow_ground', r'snow_road', r'white_road', r'snow_ground_dry_road']\n",
        "\n",
        "# path to the folder that contains sequences\n",
        "dir_splitted = 'dataset'\n",
        "\n",
        "# path to a tmp dir where sequences are sorted:\n",
        "! mkdir -p tmp/images"
      ],
      "metadata": {
        "id": "MrQP7bO3Tb3o"
      },
      "id": "MrQP7bO3Tb3o",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of sequences to label and the dict seq2images\n",
        "sequences = sorted({lbls[name]['sequence'] for name in lbls})\n",
        "\n",
        "# Create a dictionary mapping sequences to image sets\n",
        "seq2images = {\n",
        "    sequence: {name for name in lbls if lbls[name]['sequence'] == sequence}\n",
        "    for sequence in sequences\n",
        "}\n",
        "\n",
        "# List sequences that have more than one associated image\n",
        "sequences_to_sort = [seq for seq, images in seq2images.items() if len(images) > 1]\n",
        "\n",
        "print(sequences_to_sort, len(sequences_to_sort), len(sequences))\n"
      ],
      "metadata": {
        "id": "PKAolEeOUVCY",
        "outputId": "26782d50-dffe-48c4-dd42-eba92f054f3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PKAolEeOUVCY",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0000'] 1 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate new graphs dg, ug, eg:\n",
        "graphs = utils.get_new_graphs(lbls)\n",
        "tdg, tug, teg = utils.get_sdg_from_levels(graphs, lbls, param) # no supplementary edges"
      ],
      "metadata": {
        "id": "xWZLzJUGUk6w",
        "outputId": "9c923fe5-03e3-4389-dd9e-313e7df14ce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xWZLzJUGUk6w",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0000\n",
            "set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add previous comparisons (stored in labels_ord) & Cleaning : one must not have cycles\n",
        "graphs= (tdg, tug, teg)\n",
        "tdg, tug, teg = utils.fill_graph_from_splitted(graphs, dir_splitted, ext='sc')\n",
        "print(len(tdg.edges), len(tug.edges), len(teg.edges))\n",
        "\n",
        "\n",
        "#washing\n",
        "two_cycles = set(tdg.edges).intersection(utils.invert_edges(set(tdg.edges)))\n",
        "two_cycles = utils.edges_and_inverted_edges(two_cycles)\n",
        "print(two_cycles)\n",
        "#%%\n",
        "print(str(len(two_cycles)) + ' 2-cycles')\n",
        "tdg.remove_edges_from(two_cycles)\n",
        "tdg = nx.transitive_closure(tdg)\n",
        "two_cycles = set(tdg.edges).intersection(utils.invert_edges(set(tdg.edges)))\n",
        "two_cycles = utils.edges_and_inverted_edges(two_cycles)\n",
        "print(str(len(two_cycles)) + ' 2-cycles after washing')\n",
        "\n",
        "\n",
        "vs03 = utils.edges_and_inverted_edges(tug.edges).intersection(set(tdg.edges))\n",
        "print(str(len(vs03)) + ' 0vs3')\n",
        "vs03eg = {edge for edge in vs03 if edge in teg.edges}\n",
        "tdg.remove_edges_from(vs03)\n",
        "tug.remove_edges_from(vs03)\n",
        "teg.remove_edges_from(vs03eg)\n",
        "\n",
        "tdg = nx.transitive_closure(tdg)\n",
        "\n",
        "vs03 = utils.edges_and_inverted_edges(tug.edges).intersection(set(tdg.edges))\n",
        "print(str(len(vs03)) + ' 0vs3 after washing')\n",
        "print(len(tdg.edges), len(tug.edges), len(teg.edges))"
      ],
      "metadata": {
        "id": "9M4jX2HrW383",
        "outputId": "3e2a4d0c-a0b0-4fd6-f3bf-87037c8cff14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9M4jX2HrW383",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nothing in sequence: 0000\n",
            "nothing in sequence: 0\n",
            "0 0 0\n",
            "set()\n",
            "0 2-cycles\n",
            "0 2-cycles after washing\n",
            "0 0vs3\n",
            "0 0vs3 after washing\n",
            "0 0 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd annotation ; git pull ; cd ..\n",
        "# Reload each module if needed\n",
        "importlib.reload(utils)\n",
        "importlib.reload(pmf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpFWu9l4OncR",
        "outputId": "9b510bb5-a7f5-4334-898b-0636a695bac2"
      },
      "id": "EpFWu9l4OncR",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects:  25% (1/4)\u001b[K\rremote: Compressing objects:  50% (2/4)\u001b[K\rremote: Compressing objects:  75% (3/4)\u001b[K\rremote: Compressing objects: 100% (4/4)\u001b[K\rremote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  25% (1/4)\rUnpacking objects:  50% (2/4)\rUnpacking objects:  75% (3/4)\rUnpacking objects: 100% (4/4)\rUnpacking objects: 100% (4/4), 1012 bytes | 337.00 KiB/s, done.\n",
            "From https://github.com/nanopiero/annotation\n",
            "   b489266..57aef27  main       -> origin/main\n",
            "Updating b489266..57aef27\n",
            "Fast-forward\n",
            " src/poset_mergesort_functions_per_level.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'src.poset_mergesort_functions_per_level' from '/content/annotation/src/poset_mergesort_functions_per_level.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sequence in enumerate(sequences_to_sort):\n",
        "    print('sequence: ' + str(sequence), i)\n",
        "    root_dataset = dir_splitted\n",
        "    dataset = os.path.join(root_dataset, str(sequence))\n",
        "    images_dataset = join(dataset, 'images')\n",
        "    root_cs = join(dataset, r'labels_ord')\n",
        "\n",
        "    if not isdir(root_cs):\n",
        "        os.mkdir(root_cs)\n",
        "\n",
        "    # init frame_lbls and dic of subgroup\n",
        "    frame_lbls = {}\n",
        "    for name in os.listdir(images_dataset):\n",
        "            frame_lbls[name] = lbls[name]\n",
        "\n",
        "    dic_of_subgroups = {}\n",
        "    dic_of_subgroups[''] = [name for name in frame_lbls]\n",
        "\n",
        "    labeling_root= \"tmp\"\n",
        "    temp_images_dir= \"tmp/images\"\n",
        "    name_poset_sc = r'poset_sc.pickle'\n",
        "\n",
        "    #%def graph paths\n",
        "    graphs_names = [os.path.join(graph + '_sc' +r'.gpickle') for graph in [\"dg\",\"ug\",\"eg\"]]\n",
        "    graphs_paths = [os.path.join(root_cs, graph_name) for graph_name in graphs_names]\n",
        "\n",
        "    #def nodes\n",
        "    nodes = sorted(dic_of_subgroups[subgroup])\n",
        "    print('labelling of ' + str(len(nodes)) + ' nodes')\n",
        "\n",
        "    #clean images_dir\n",
        "    for name in os.listdir(temp_images_dir):\n",
        "        os.remove(os.path.join(temp_images_dir,name))\n",
        "\n",
        "    #copy in the temp image dir\n",
        "    for node in nodes:\n",
        "        shutil.copy(os.path.join(dataset, 'images', node), os.path.join(temp_images_dir))\n",
        "\n",
        "    # if graphs_names[0] not in os.listdir(root_cs):\n",
        "    print(\"need to init graphs\")\n",
        "    # restrict tdg, tug, teg:\n",
        "    dg = tdg.subgraph(nodes).copy()\n",
        "    ug = tug.subgraph(nodes).copy()\n",
        "    eg = teg.subgraph(nodes).copy()\n",
        "\n",
        "    #save the graphs\n",
        "    utils.write_gpickle(dg, graphs_paths[0])\n",
        "    utils.write_gpickle(ug, graphs_paths[1])\n",
        "    utils.write_gpickle(eg, graphs_paths[2])\n",
        "\n",
        "    # else:\n",
        "    #     pass\n",
        "\n",
        "\n",
        "    kwargs = {'root_cs': root_cs,\n",
        "              'critere':  'sc',\n",
        "              'subgroup': subgroup,\n",
        "              'mode' : '',\n",
        "              'graphs_paths': graphs_paths,\n",
        "              'images_dir' : temp_images_dir\n",
        "              }\n",
        "\n",
        "    decomposition = pmf.labelling_mode(lbls, **kwargs)"
      ],
      "metadata": {
        "id": "MajnXsMOgFv3"
      },
      "id": "MajnXsMOgFv3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name1 = 'dataset/0000/images/0.jpg'\n",
        "name2 = 'dataset/0000/images/1.jpg'\n"
      ],
      "metadata": {
        "id": "5ghcDWA1vdQ8"
      },
      "id": "5ghcDWA1vdQ8",
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "\n",
        "def compare(lbls, name1, name2, mode, images_dir, critere, **kwargs):\n",
        "    image_path1 = os.path.join(images_dir, name1)\n",
        "    image_path2 = os.path.join(images_dir, name2)\n",
        "\n",
        "    im1 = Image.open(image_path1).resize((1000, 1000))\n",
        "    im2 = Image.open(image_path2).resize((1000, 1000))\n",
        "\n",
        "    def show_image(image_num):\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        if image_num == 0:\n",
        "            plt.title(f'Image 0: {name1[-10:-4]} {lbls[name1][\"level\" + critere]}')\n",
        "            plt.imshow(im1)\n",
        "        else:\n",
        "            plt.title(f'Image 1: {name2[-10:-4]} {lbls[name2][\"level\" + critere]}')\n",
        "            plt.imshow(im2)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    interact(show_image, image_num=(0, 1))"
      ],
      "metadata": {
        "id": "Xi66Yk0TvFAl"
      },
      "id": "Xi66Yk0TvFAl",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare(lbls, '0.jpg', '1.jpg', '', 'dataset/0000/images', 'sc')"
      ],
      "metadata": {
        "id": "VAK6bcrxvI80"
      },
      "id": "VAK6bcrxvI80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean:\n",
        "#00010268_0_20131114_081603.jpg\n",
        "ntr = ['00010657_7002_20130721_180838.jpg']\n",
        "\n",
        "clean_nodes(ntr, **kwargs)\n",
        "\n",
        "#%%\n",
        "path2 = os.path.join(root_cs, 'decomposition_vvday.pickle')\n",
        "os.remove(path2)"
      ],
      "metadata": {
        "id": "moggg4j1gGtG"
      },
      "id": "moggg4j1gGtG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for name in os.listdir(splitted_dir):\n",
        "#    path = os.path.join(splitted_dir,name,'labels_ord')\n",
        "#    if 'decomposition_snowday_surface.pickle' in os.listdir(path):\n",
        "#        path2 = os.path.join(path, 'decomposition_snowday_surface.pickle')\n",
        "#        os.remove(path2)"
      ],
      "metadata": {
        "id": "LXi6QR2PgJZm"
      },
      "id": "LXi6QR2PgJZm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autres éléments dans lesquels piquer évzntuellement :\n",
        "\n",
        "\n",
        "def get_dg2vvday(root_cs):\n",
        "\n",
        "    path_of_dg2 = os.path.join(root_cs, \"poset_vvday\" + r\".gpickle\" )\n",
        "    dg2 = nx.read_gpickle(path_of_dg2)\n",
        "    return dg2\n",
        "\n",
        "def save_dg2vvday(dg2, root_cs):\n",
        "\n",
        "    path_of_dg2 = os.path.join(root_cs, \"poset_vvday\" + r\".gpickle\" )\n",
        "    nx.write_gpickle(dg2, path_of_dg2)\n",
        "    print(\"graph dg2 saved\")\n",
        "\n",
        "def save_graphs(graphs, graphs_paths, **kwargs):\n",
        "    dg, ug, eg = graphs\n",
        "    nx.write_gpickle(dg, graphs_paths[0])\n",
        "    nx.write_gpickle(ug, graphs_paths[1])\n",
        "    nx.write_gpickle(eg, graphs_paths[2])\n",
        "    print(\"graphs saved\")\n",
        "\n",
        "doublons  =[]\n",
        "for i, sequence in enumerate(sequences_to_sort):  #[-42:-41]:\n",
        "\n",
        "    root_dataset = dir_splitted\n",
        "    #cam = r\"nancy2\"\n",
        "\n",
        "\n",
        "    dataset = os.path.join(root_dataset, str(sequence))\n",
        "    images_dataset = join(dataset, 'images')\n",
        "    labels_dataset = join(dataset, 'labels')\n",
        "    root_cs = join(dataset,r'labels_ord')\n",
        "\n",
        "\n",
        "    for name in os.listdir(images_dataset):\n",
        "        if 'Copie' in name:\n",
        "            print('sequence: ' + str(sequence), i, name)\n",
        "\n",
        "            correct_name = name.split(' - ')[0] + '.jpg'\n",
        "            level = copy.deepcopy(corrected_lbls[name])\n",
        "\n",
        "\n",
        "            print(correct_name, level, correct_name in os.listdir(images_dataset))\n",
        "\n",
        "            if correct_name in os.listdir(images_dataset):\n",
        "                doublons.append(name)\n",
        "\n",
        "            else:\n",
        "                corrected_lbls[correct_name] = level\n",
        "#\n",
        "            del corrected_lbls[name]\n",
        "\n",
        "# Vérification:\n",
        "for name in corrected_lbls:\n",
        "    if 'Copie' in name:\n",
        "        print(name)\n",
        "\n",
        "#%% correction des graphs:\n",
        "for i, sequence in enumerate(sequences_to_sort):\n",
        "    modify_graph = False\n",
        "    root_dataset = dir_splitted\n",
        "    #cam = r\"nancy2\"\n",
        "\n",
        "\n",
        "    dataset = os.path.join(root_dataset, str(sequence))\n",
        "    images_dataset = join(dataset, 'images')\n",
        "    labels_dataset = join(dataset, 'labels')\n",
        "    root_cs = join(dataset,r'labels_ord')\n",
        "    name_poset_vv = r'poset_vv.pickle'\n",
        "    name_poset_snowsurface =r'poset_snow_surface.pickle'\n",
        "    name_poset_snowheight = r'poset_snow_height.pickle'\n",
        "\n",
        "    #%def graph paths\n",
        "    suffixe = get_suffixe(critere,subgroup,mode)\n",
        "    graphs_names = [os.path.join(graph + suffixe+r'.gpickle') for graph in [\"dg\",\"ug\",\"eg\"]]\n",
        "    graphs_paths = [os.path.join(root_cs, graph_name) for graph_name in graphs_names]\n",
        "\n",
        "\n",
        "    for name in os.listdir(images_dataset):\n",
        "        if 'Copie' in name:\n",
        "            modify_graph = True\n",
        "\n",
        "    if modify_graph:\n",
        "\n",
        "        dg, ug, eg = get_graphs(graphs_paths)\n",
        "        dg2 = get_dg2vvday(root_cs)\n",
        "\n",
        "        for name in os.listdir(images_dataset):\n",
        "            if 'Copie' in name:\n",
        "\n",
        "                if name in doublons:\n",
        "                    print(name + ' was in doublons')\n",
        "                    dg.remove_node(name)\n",
        "                    ug.remove_node(name)\n",
        "                    eg.remove_node(name)\n",
        "                    dg2.remove_node(name)\n",
        "                else:\n",
        "                    correct_name = name.split(' - ')[0] + '.jpg'\n",
        "                    mapping = {name:correct_name}\n",
        "                    print(sequence, name, len(dg.nodes))\n",
        "                    dg = nx.relabel_nodes(dg, mapping)\n",
        "                    ug = nx.relabel_nodes(ug, mapping)\n",
        "                    eg = nx.relabel_nodes(eg, mapping)\n",
        "                    dg2 = nx.relabel_nodes(dg2, mapping)\n",
        "                    print(name, len(dg.nodes))\n",
        "\n",
        "        save_graphs((dg, ug, eg), graphs_paths)\n",
        "        save_dg2vvday(dg2, root_cs)\n",
        "\n",
        "#%% Vérification:\n",
        "\n",
        "for i, sequence in enumerate(sequences_to_sort):\n",
        "    modify_graph = False\n",
        "    root_dataset = dir_splitted\n",
        "    #cam = r\"nancy2\"\n",
        "\n",
        "\n",
        "    dataset = os.path.join(root_dataset, str(sequence))\n",
        "    images_dataset = join(dataset, 'images')\n",
        "    labels_dataset = join(dataset, 'labels')\n",
        "    root_cs = join(dataset,r'labels_ord')\n",
        "\n",
        "    #%def graph paths\n",
        "    suffixe = get_suffixe(critere, subgroup, mode)\n",
        "    graphs_names = [os.path.join(graph + suffixe + r'.gpickle') \\\n",
        "                    for graph in [\"dg\",\"ug\",\"eg\"]]\n",
        "    graphs_paths = [os.path.join(root_cs, graph_name) \\\n",
        "                    for graph_name in graphs_names]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    dg, ug, eg = get_graphs(graphs_paths)\n",
        "    dg2 = get_dg2vvday(root_cs)\n",
        "\n",
        "    for g in [dg, ug, eg, dg2]:\n",
        "        for n in g.nodes:\n",
        "            if \"Copie\" in n:\n",
        "                print(n)\n",
        "\n",
        "#%% renommer les images\n",
        "for i, sequence in enumerate(sequences_to_sort):  #[-42:-41]:\n",
        "\n",
        "    root_dataset = dir_splitted\n",
        "    #cam = r\"nancy2\"\n",
        "\n",
        "\n",
        "    dataset = os.path.join(root_dataset, str(sequence))\n",
        "    images_dataset = join(dataset, 'images')\n",
        "    labels_dataset = join(dataset, 'labels')\n",
        "    root_cs = join(dataset,r'labels_ord')\n",
        "\n",
        "\n",
        "    for name in os.listdir(images_dataset):\n",
        "        if 'Copie' in name:\n",
        "            print('sequence: ' + str(sequence), i, name)\n",
        "\n",
        "            correct_name = name.split(' - ')[0] + '.jpg'\n",
        "\n",
        "\n",
        "            if name in doublons:\n",
        "                print(\"delete name\")\n",
        "                os.remove(join(images_dataset, name))\n",
        "            else:\n",
        "                src = join(images_dataset, name)\n",
        "                dst = join(images_dataset, correct_name)\n",
        "                print(\"move \" + src + \" to \" + dst)\n",
        "                shutil.move(src,dst)\n",
        "\n",
        "\n",
        "#%% Vérif que correct_lbls contient toutes les images:\n",
        "for i, sequence in enumerate(sequences_to_sort):  #[-42:-41]:\n",
        "\n",
        "    root_dataset = dir_splitted\n",
        "    #cam = r\"nancy2\"\n",
        "\n",
        "\n",
        "    dataset = os.path.join(root_dataset, str(sequence))\n",
        "    images_dataset = join(dataset, 'images')\n",
        "    labels_dataset = join(dataset, 'labels')\n",
        "    root_cs = join(dataset,r'labels_ord')\n",
        "\n",
        "    seq_names = set([n for n in corrected_lbls if corrected_lbls[n]['sequence'] == sequence])\n",
        "\n",
        "    if not seq_names == set(os.listdir(images_dataset)):\n",
        "        print(seq)\n",
        "\n",
        "#%% sauvegarde de corrected_labels:\n",
        "\n",
        "label_path2 = os.path.join(root,\n",
        "                    r'labels_imagewise_BAMOSvv_corr2.pickle')\n",
        "\n",
        "\n",
        "with open(label_path2, 'wb') as f:\n",
        "    pickle.dump(corrected_lbls, f)\n",
        "\n",
        "\n",
        "#%% Fabrication des ug2:\n",
        "def save_ug2vvday(ug2, root_cs):\n",
        "\n",
        "    path_of_ug2 = os.path.join(root_cs, \"posetbar_vvday\" + r\".gpickle\" )\n",
        "    nx.write_gpickle(ug2, path_of_ug2)\n",
        "    print(\"graph ug2 saved\")\n",
        "\n",
        "\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for i, sequence in enumerate(sequences_to_sort):\n",
        "    modify_graph = False\n",
        "    root_dataset = dir_splitted\n",
        "    #cam = r\"nancy2\"\n",
        "\n",
        "\n",
        "    dataset = os.path.join(root_dataset, str(sequence))\n",
        "    images_dataset = join(dataset, 'images')\n",
        "    labels_dataset = join(dataset, 'labels')\n",
        "    root_cs = join(dataset,r'labels_ord')\n",
        "    name_poset_vv = r'poset_vv.pickle'\n",
        "    name_poset_snowsurface =r'poset_snow_surface.pickle'\n",
        "    name_poset_snowheight = r'poset_snow_height.pickle'\n",
        "\n",
        "    #%def graph paths\n",
        "    suffixe = get_suffixe(critere,subgroup,mode)\n",
        "    graphs_names = [os.path.join(graph + suffixe+r'.gpickle') for graph in [\"dg\",\"ug\",\"eg\"]]\n",
        "    graphs_paths = [os.path.join(root_cs, graph_name) for graph_name in graphs_names]\n",
        "\n",
        "\n",
        "    dg, ug, eg = get_graphs(graphs_paths)\n",
        "    dg2 = get_dg2vvday(root_cs)\n",
        "\n",
        "    # test que dg2 est bien construit\n",
        "    if len(set(dg2.nodes) - set(dg.nodes)) > 0:\n",
        "        print(sequence)\n",
        "    # (en fait, pour trois séquences, ça a merdouillé\n",
        "    # - sans doute manip de decomp. dans ce cas, on ne ug2 = ug)\n",
        "\n",
        "    if len(nx.transitive_closure(dg2).edges) < len(nx.transitive_closure(dg).edges):\n",
        "        print(sequence)\n",
        "        ug2 = ug\n",
        "        dg2 = dg\n",
        "        save_dg2vvday(dg2, root_cs)\n",
        "        save_ug2vvday(ug2, root_cs)\n",
        "\n",
        "    else:\n",
        "        tdg = nx.transitive_closure(dg)\n",
        "        tdgu = tdg.to_undirected()\n",
        "        ug2_ = nx.complement(tdgu)\n",
        "\n",
        "        # vérif que ug2_ contient bien ug (aux auto edges près)\n",
        "        for e in ug.edges:\n",
        "            if not ug2_.has_edge(e[0], e[1]) and (e[0] != e[1]):\n",
        "                print('aië')\n",
        "\n",
        "\n",
        "        # ajout des edges de ug2_\n",
        "        new_ug_edges = set([(e[0], e[1], -1) for e in ug2_.edges\\\n",
        "                         if not ug.has_edge(*e)])\n",
        "        counter += len(new_ug_edges)\n",
        "        print(len(new_ug_edges))\n",
        "\n",
        "        ug.add_weighted_edges_from(new_ug_edges)\n",
        "\n",
        "        save_ug2vvday(ug, root_cs)\n",
        "\n",
        "print(counter)\n",
        "\n",
        "#%% Vérification que les arêtes additionnelles\n",
        "# sont correctes:\n",
        "\n",
        "liste_verif = [('00010618_6_20161209_150114.jpg', '00010618_6_20161201_103013.jpg', -1),\n",
        "('00010621_0_20120109_143302.jpg', '00010621_0_20140621_100310.jpg', -1),\n",
        "('00010628_7_20120107_090956.jpg', '00010628_7_20120107_080955.jpg', -1),\n",
        "('00010628_7_20120107_090956.jpg', '00010628_7_20120107_073955.jpg', -1),\n",
        "('00010628_7_20120107_073955.jpg', '00010628_7_20120107_080955.jpg', -1),\n",
        "('00010657_7002_20130925_103845.jpg', '00010657_7002_20130721_180838.jpg', -1),\n",
        "('00010660_36_20140417_054134.jpg', '00010660_36_20140524_184135.jpg', -1),\n",
        "('00010726_5_20150518_091628.jpg', '00010726_5_20150415_174635.jpg', -1),\n",
        "('00010853_0_20100812_170528.jpg', '00010853_0_20110128_095112.jpg', -1),\n",
        "('00010853_0_20110508_142110.jpg', '00010853_0_20110710_085110.jpg', -1),\n",
        "('00010871_0_20110120_123852.jpg', '00010871_0_20110126_113854.jpg', -1),\n",
        "('00010874_19_20131211_121201.jpg', '00010874_19_20131220_124158.jpg', -1),\n",
        "('00010926_53_20140521_180312.jpg', '00010926_53_20140609_173316.jpg', -1),\n",
        "('00011287_0_20100504_022356.jpg', '00011287_0_20100401_201949.jpg', -1),\n",
        "('00011287_0_20100305_211757.jpg', '00011287_0_20100522_171919.jpg', -1),\n",
        "('00011351_1_20150121_220130.jpg', '00011351_1_20161005_210131.jpg', -1),\n",
        "('00011351_1_20150121_220130.jpg', '00011351_1_20170107_170127.jpg', -1),\n",
        "('00011377_2_20100718_170031.jpg', '00011377_2_20100531_140021.jpg', -1),\n",
        "('00011385_10001_20150418_120436.jpg', '00011385_10001_20150430_153441.jpg', -1),\n",
        "('00011385_10001_20150418_120436.jpg', '00011385_10001_20150504_203437.jpg', -1)]\n",
        "compas = []\n",
        "for e in liste_verif:\n",
        "    name0 = e[0]\n",
        "    name1 = e[1]\n",
        "    sequence = e[0].split('_')[0] + '_' + e[0].split('_')[1]\n",
        "    dataset = os.path.join(root_dataset, sequence)\n",
        "    images_dataset = join(dataset, 'images')\n",
        "    compas.append(compare(name0, name1, mode, images_dataset, critere))\n",
        "#%% images fully noisy\n",
        "\n",
        "root = r\"D:\\sniff_webcams\\AMOS\"\n",
        "\n",
        "dir_noisy_images = os.path.join(root,\n",
        "                          r\"full_noisy_images\")\n",
        "\n",
        "\n",
        "\n",
        "# 1: Ajout de toutes les images annotées 'f' dans bamosvv2 dans le pool\n",
        "for name_img in corrected_lbls:\n",
        "    if 'f' in corrected_lbls[name_img]['levelvv']:\n",
        "        sequence = name_img.split('_')[0] + '_' + name_img.split('_')[1]\n",
        "        seq = corrected_lbls[name_img]['sequence']\n",
        "        src = join(dir_splitted, seq, 'images', name_img)\n",
        "        dst = join(dir_noisy_images, name_img)\n",
        "        shutil.copy(src,dst)\n",
        "\n",
        "# à la main: sélection des images annotées 'f' universelles (ie: ne comportant aucune info, et ne correspondant pas à un changement de scène)\n",
        "# -> nuit noires, et surtout caméras \"bouchées\".\n",
        "#%% Construction de BAMOSvv3: images/graphe\n",
        "dir_bamos = os.path.join(root,\n",
        "                            'BAMOSvv3')\n",
        "dir_bamos_images = join(dir_bamos, 'images')\n",
        "\n",
        "for sequence in os.listdir(dir_splitted):\n",
        "    dir_sequence = join(dir_splitted, sequence, 'images')\n",
        "\n",
        "\n",
        "    images = os.listdir(dir_sequence)\n",
        "\n",
        "    for name_image in images:\n",
        "        src = join(dir_sequence, name_image)\n",
        "        dst = join(dir_bamos_images, name_image)\n",
        "        shutil.copy(src, dst)\n",
        "# 5374 images\n",
        "#%% 2) pool_BAMOSvv2 -> BAMOSvv3\n",
        "\n",
        "\n",
        "dir_pool_BAMOSvv = os.path.join(root,\n",
        "                            'pool_BAMOSvv2')\n",
        "\n",
        "for sequence in os.listdir(dir_pool_BAMOSvv):\n",
        "    dir_pool_images = join(dir_pool_BAMOSvv, sequence, 'images')\n",
        "    dir_sequence = join(dir_splitted, sequence, 'images')\n",
        "\n",
        "    try:\n",
        "        images = os.listdir(dir_sequence)\n",
        "    except:\n",
        "        images = []\n",
        "\n",
        "    for name_image in os.listdir(dir_pool_images):\n",
        "        if (not 'Copie' in name_image) and (name_image not in images):\n",
        "            src = join(dir_pool_images, name_image)\n",
        "            dst = join(dir_bamos_images, name_image)\n",
        "            shutil.copy(src, dst)\n",
        "\n",
        "#%% 3) full noisy images -> BAMOSvv3\n",
        "dir_noisy_images = os.path.join(root,\n",
        "                          r\"full_noisy_images\")\n",
        "\n",
        "for name_image in os.listdir(dir_noisy_images):\n",
        "    src = join(dir_noisy_images, name_image)\n",
        "    dst = join(dir_bamos_images, name_image)\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "# 41736 images\n",
        "#%% Construction des graphs complets:\n",
        "\n",
        "# 1) Aggrégation des graphes venant de BAMOSvv2:\n",
        "\n",
        "tdg = nx.DiGraph()\n",
        "tug = nx.Graph()\n",
        "teg = nx.Graph()\n",
        "\n",
        "for sequence in os.listdir(dir_splitted):\n",
        "    try:\n",
        "        dir_labels = os.path.join(dir_splitted,sequence,'labels_ord')\n",
        "        # attention: poset_vvday a perdu les poids. Mais ils sont dans dg_vv\n",
        "        # qui contient la même information\n",
        "        dg_path = os.path.join(dir_labels, 'dg_vvday.gpickle')\n",
        "        ug_path = os.path.join(dir_labels, 'posetbar_vvday.gpickle')\n",
        "        eg_path = os.path.join(dir_labels, 'eg_vvday.gpickle')\n",
        "\n",
        "        local_dg = nx.read_gpickle(dg_path)\n",
        "        local_ug = nx.read_gpickle(ug_path)\n",
        "        local_eg = nx.read_gpickle(eg_path)\n",
        "    #            print(len(local_ug.edges))\n",
        "\n",
        "        tdg = nx.compose(tdg, local_dg)\n",
        "        tug = nx.compose(tug, local_ug)\n",
        "        teg = nx.compose(teg, local_eg)\n",
        "\n",
        "\n",
        "    except:\n",
        "        pass\n",
        "        print('nothing in sequence: ' + str(sequence))\n",
        "\n",
        "\n",
        "\n",
        "#%% Contrôle:\n",
        "\n",
        "print(len(tdg.edges), len(tug.edges), len(teg.edges))\n",
        "\n",
        "\n",
        "#washing\n",
        "two_cycles = set(tdg.edges).intersection(invert_edges(set(tdg.edges)))\n",
        "two_cycles = edges_and_inverted_edges(two_cycles)\n",
        "print(str(len(two_cycles)) + ' 2-cycles')\n",
        "\n",
        "\n",
        "tdg.remove_edges_from(two_cycles)\n",
        "tdg = nx.transitive_closure(tdg)\n",
        "two_cycles = set(tdg.edges).intersection(invert_edges(set(tdg.edges)))\n",
        "two_cycles = edges_and_inverted_edges(two_cycles)\n",
        "print(str(len(two_cycles)) + ' 2-cycles after washing')\n",
        "\n",
        "\n",
        "vs03 = edges_and_inverted_edges(tug.edges).intersection(set(tdg.edges))\n",
        "print(str(len(vs03)) + ' 0vs3')\n",
        "\n",
        "\n",
        "vs03eg = {edge for edge in vs03 if edge in teg.edges}\n",
        "vs03 = edges_and_inverted_edges(tug.edges).intersection(set(tdg.edges))\n",
        "tdg.remove_edges_from(vs03)\n",
        "tug.remove_edges_from(vs03)\n",
        "teg.remove_edges_from(vs03eg)\n",
        "print(str(len(vs03)) + ' 0vs3 after washing')\n",
        "print(len(tdg.edges), len(tug.edges), len(teg.edges))\n",
        "\n",
        "\n",
        "# 26841 12489 3373\n",
        "#%% Ajout des edges \"noisy\"\n",
        "images = sorted(os.listdir(dir_bamos_images))\n",
        "new_edges = []\n",
        "for noisy_image in os.listdir(dir_noisy_images):\n",
        "    sequence = noisy_image.split('_')[0]\n",
        "    same_seq_images = [img for img in images if img.split('_')[0] == sequence]\n",
        "    new_edges += [(noisy_image, img, -2) for img in same_seq_images]\n",
        "\n",
        "print(len(new_edges)) #141138\n",
        "tug.add_weighted_edges_from(new_edges)\n",
        "#%% contrôle\n",
        "vs03 = edges_and_inverted_edges(tug.edges).intersection(set(tdg.edges))\n",
        "print(str(len(vs03)) + ' 0vs3')\n",
        "tdg.remove_edges_from(vs03)\n",
        "#%%\n",
        "print(len(tdg.edges), len(tug.edges), len(teg.edges))\n",
        "#26839 150096 3373\n",
        "#%% Enregistrement des graphes\n",
        "\n",
        "path_tdg = os.path.join(dir_bamos, 'tdg_bamosvv_041123')\n",
        "path_tug = os.path.join(dir_bamos, 'tug_bamosvv_041123')\n",
        "path_teg = os.path.join(dir_bamos, 'teg_bamosvv_041123')\n",
        "\n",
        "nx.write_gpickle(tdg, path_tdg)\n",
        "nx.write_gpickle(tug, path_tug)\n",
        "nx.write_gpickle(teg, path_teg)\n",
        ""
      ],
      "metadata": {
        "id": "qKD8mZ9KgsfH"
      },
      "id": "qKD8mZ9KgsfH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}